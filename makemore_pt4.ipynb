{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkCh8o9OaV1KfY7ypZ3Prq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcleantom/makemore_from_scratch/blob/main/makemore_pt4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.youtube.com/watch?v=q8SA3rM6ckI&t=7s"
      ],
      "metadata": {
        "id": "_bCSw9KEoLXZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhJNrZn4oH3O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWhLDGcap0yN",
        "outputId": "6e38a857-67d2-4386-d795-8b1621787636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-18 10:57:45--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.3’\n",
            "\n",
            "\rnames.txt.3           0%[                    ]       0  --.-KB/s               \rnames.txt.3         100%[===================>] 222.80K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-18 10:57:45 (10.9 MB/s) - ‘names.txt.3’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr8yrjacp1bP",
        "outputId": "826c8dc4-38db-47ba-8dd4-64c9cb115162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0AxS2Kcp3fZ",
        "outputId": "2bfae440-aaf4-40a7-d693-a5fed74d2974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):  \n",
        "  X, Y = [], []\n",
        "  \n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdhLU4Owp5B4",
        "outputId": "99e897e9-80f2-4ff6-ea3c-e6a8bd4a0af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "_vA9wVP5p9n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bve4NMA6p_4q",
        "outputId": "db5f9191-6a21-4cb1-b1a0-a0bd72ad6a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ],
      "metadata": {
        "id": "UEKaluuBqBkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icuFDtO8qS_h",
        "outputId": "ae4fbb8c-506b-422a-b254-16c8f7c0e4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3530, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logprobs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8PS-XRWrLaM",
        "outputId": "3858c6a4-4b38-4d82-d15d-52efdba68de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Excercise 1 backprop through the whole thing manually\n",
        "\n",
        "#loss = (a+b+c)/3 = 1/3a + 1/3b + 1/3c -> dloss/da = 1/3\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0 / n\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "\n",
        "# dlog/dx = 1/x\n",
        "# chain rule dprobs/dloss = dprobs/dlogprobs * dlogprobs/dloss\n",
        "dprobs = (1.0/probs) * dlogprobs\n",
        "cmp('probs', dprobs, probs)\n",
        "\n",
        "# c = a * b -> dc/db = a\n",
        "# because counts_sum_inv is broadcasted, we need to sum the gradients across the\n",
        "# broadcasted dim\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "\n",
        "# y = x**-1 => dy/dx = -x**-2\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "cmp('counts', dcounts, counts)\n",
        "\n",
        "dnorm_logits = norm_logits.exp() * dcounts\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "cmp('logits', dlogits, logits)\n",
        "\n",
        "\"\"\"\n",
        "[d11, d12     [a11, a12     [b11, b12     [c1\n",
        " d21, d22] =   a12, a11] @   b12, b22]  +  c2]\n",
        "\n",
        "\n",
        "dL/da = dl/dd @ b^T\n",
        "dL/db = a^T @ dl/dd\n",
        "dl/dc = dl/dd . sum(0)\n",
        "\"\"\"\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "\n",
        "#y = tan(x) => dy/dx = (1-y**2)\n",
        "dhpreact = (1.0 - h**2) * dh\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "\n",
        "dbndiff = (bnvar_inv * dbnraw)\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "\n",
        "dbnvar = (-0.5*(bnvar+1e-5)**-1.5)*dbnvar_inv\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "\n",
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "\n",
        "dbndiff += (2*bndiff) * dbndiff2\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "\n",
        "dhprebn = dbndiff.clone()\n",
        "dbnmeani = (-dbndiff).sum(0)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "\n",
        "dhprebn +=  (1/n)*(torch.ones_like(hprebn)*dbnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "\n",
        "demb = dembcat.view(emb.shape)\n",
        "cmp('emb', demb, emb)\n",
        "\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k, j]\n",
        "    dC[ix] += demb[k, j]\n",
        "cmp('C', dC, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8YEZOolqg6y",
        "outputId": "ec38824b-9610-4044-978c-d810eda1daee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2\n",
        "\n",
        "dlogits = F.softmax(logits, 1)\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "\n",
        "cmp('logits', dlogits, logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1SVf2RcYICP",
        "outputId": "5280985f-6ed5-4db5-db20-bcc20156ee16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 5.3551048040390015e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UTKeEWvgihU",
        "outputId": "3b1ddb4a-80fc-467b-9650-05c2c22e4b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.3970e-09, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')\n",
        "# The correct character has a -ve gradient, incorrect +ve"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "ENiZPGPggZ5t",
        "outputId": "8403fdbb-8b4c-4c76-ec09-3be413cb2b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff9cf3cd340>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5bvVlj3hMQs",
        "outputId": "242e640c-1371-420e-bf26-f55bc0a9774e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0021,  0.0028,  0.0006,  0.0017,  0.0007,  0.0025,  0.0008,  0.0011,\n",
              "        -0.0308,  0.0010,  0.0011,  0.0011,  0.0011,  0.0009,  0.0011,  0.0004,\n",
              "         0.0003,  0.0006,  0.0005,  0.0016,  0.0017,  0.0007,  0.0008,  0.0023,\n",
              "         0.0018,  0.0008,  0.0007], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vkh8y9AegN_j",
        "outputId": "a0538826-de06-493c-80b7-e046a2861baa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BoZOhblheZI",
        "outputId": "29293de5-c56c-4cd2-d67d-44f4877e75e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "  # kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    # loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    # YOUR CODE HERE :)\n",
        "    dlogits = F.softmax(logits, 1)\n",
        "    dlogits[range(n), Yb] -= 1\n",
        "    dlogits /= n\n",
        "    # 2nd layer backprop\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    # tanh\n",
        "    dhpreact = (1.0 - h**2) * dh\n",
        "    # batchnorm backprop\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "    # 1st layer\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "    # embedding\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for k in range(Xb.shape[0]):\n",
        "      for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k,j]\n",
        "        dC[ix] += demb[k,j]\n",
        "\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "\n",
        "    # if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "    #   break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvKcPvvSi8sA",
        "outputId": "63b39d34-139b-4412-8890-faf29f4c44a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7662\n",
            "  10000/ 200000: 2.1989\n",
            "  20000/ 200000: 2.3633\n",
            "  30000/ 200000: 2.4901\n",
            "  40000/ 200000: 2.0040\n",
            "  50000/ 200000: 2.3955\n",
            "  60000/ 200000: 2.3837\n",
            "  70000/ 200000: 1.9980\n",
            "  80000/ 200000: 2.3292\n",
            "  90000/ 200000: 2.1342\n",
            " 100000/ 200000: 2.0150\n",
            " 110000/ 200000: 2.3160\n",
            " 120000/ 200000: 1.9951\n",
            " 130000/ 200000: 2.4288\n",
            " 140000/ 200000: 2.3006\n",
            " 150000/ 200000: 2.1296\n",
            " 160000/ 200000: 1.9540\n",
            " 170000/ 200000: 1.8490\n",
            " 180000/ 200000: 2.0017\n",
            " 190000/ 200000: 1.8942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p, g in zip(parameters, grads):\n",
        "  cmp(str(tuple(p.shape)), g, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWURDo3gjZor",
        "outputId": "7ccba25f-49a4-4494-a6df-a3884e4aa596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27, 10)        | exact: False | approximate: True  | maxdiff: 1.0244548320770264e-08\n",
            "(30, 200)       | exact: False | approximate: True  | maxdiff: 8.381903171539307e-09\n",
            "(200,)          | exact: False | approximate: True  | maxdiff: 3.958120942115784e-09\n",
            "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n",
            "(27,)           | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-09\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
      ],
      "metadata": {
        "id": "02zcjFQBjo9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCWxq-Ykj5dT",
        "outputId": "88683ffc-4817-44cc-f74c-51f9b6a64f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.0704824924468994\n",
            "val 2.1111085414886475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "    \n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # ------------\n",
        "      # forward pass:\n",
        "      # Embedding\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # ------------\n",
        "      # Sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "    \n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0tJLpCjj9ze",
        "outputId": "076cac3d-b60c-458d-dd71-4e831a61c7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "carmahzamille.\n",
            "khi.\n",
            "mreigentleenanden.\n",
            "jazhnen.\n",
            "delynn.\n",
            "jareen.\n",
            "nellara.\n",
            "chaiivon.\n",
            "leigh.\n",
            "ham.\n",
            "joce.\n",
            "quinn.\n",
            "shoilea.\n",
            "jadilyn.\n",
            "jero.\n",
            "dearyxia.\n",
            "kael.\n",
            "dura.\n",
            "med.\n",
            "edi.\n"
          ]
        }
      ]
    }
  ]
}